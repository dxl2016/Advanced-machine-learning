* Product rule, sum rule, Bayes's theorem, likelihood maximization
* Classification, clustering, and regression tasks in machine learning
* Loss functions, training vs inference, overfitting problem
* Optimization techniques, e.g. (stochastic) gradient descent
* Deep Learning architectures, e.g. Recurrent and Convolutional Neural Networks
* Python programming and willingness to learn new tool, e.g. Tensorflow.

text pre-preocessing, sentiment analysis, text classification
* tokenization, NLTK, normalization (stemming/lemmatization), casing, acronyms, etc.
* feature extraction: BOW, add orders, n-grams, filter out bad n-grams, tf-idf with row normalization

* linear models over BOW scale well, e.g., logistic regression + 1-gram
* spam filtering, hash functions, personalized
* vowpal wabbit

* one-hot vectors + NN (dense) rather than BOW (sparse) using word2vec embeddings
* n-gram conv filtering provides high activations, sliding windows, can learn some higher meaningful features than 1-gram, called 1Dconv, words occur on time axis, take the max activations of filters to reduce variable size to 1



* using Google Colab to open notebooks in a GitHub repository





